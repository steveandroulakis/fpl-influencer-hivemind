"""YouTube video discovery utilities used by the hivemind pipeline."""

import argparse
import json
import logging
import os
import re
from collections.abc import Sequence
from dataclasses import dataclass, field
from datetime import UTC, datetime, timedelta
from pathlib import Path
from typing import Any, ClassVar

import anthropic
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from pydantic import BaseModel, ValidationError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
DEFAULT_CHANNELS_CONFIG = PROJECT_ROOT / "youtube-titles" / "channels.json"
DEFAULT_ANTHROPIC_MODEL = "claude-3-5-haiku-20241022"
DEFAULT_TEMPERATURE = 0.0


class VideoPickerError(RuntimeError):
    """Raised when video discovery cannot produce a result."""


@dataclass
class VideoItem:
    """Represents a YouTube video with metadata."""

    title: str
    url: str
    published_at: datetime
    channel_name: str
    description: str = ""
    normalized_title: str = field(init=False)

    def __post_init__(self) -> None:
        self.normalized_title = self.normalize_text(self.title)

    @staticmethod
    def normalize_text(text: str) -> str:
        """Normalize text for matching - lowercase, strip whitespace."""
        return text.lower().strip()


class AnthropicChannelResponse(BaseModel):
    """Pydantic model for single channel response."""

    channel_name: str
    chosen_index: int
    chosen_url: str
    confidence: float
    matched_signals: list[str]
    reasoning: str


class AnthropicResponse(BaseModel):
    """Pydantic model for Anthropic API response validation."""

    channels: list[AnthropicChannelResponse]


@dataclass
class ChannelResult:
    """Result of video selection for a single channel."""

    channel_name: str
    channel_url: str
    picked: VideoItem | None
    alternatives: list[VideoItem]
    confidence: float = 0.0
    reasoning: str = ""
    matched_signals: list[str] = field(default_factory=list)
    heuristic_score: float = 0.0


@dataclass
class SelectionResult:
    """Final result of video selection process for all channels."""

    channel_results: list[ChannelResult]
    gameweek: int | None
    generated_at: datetime
    model: str
    heuristic_notes: dict[str, Any]


def select_single_channel(
    *,
    channel_name: str,
    channel_url: str,
    gameweek: int | None,
    days_back: int,
    max_per_channel: int,
    anthropic_model: str = DEFAULT_ANTHROPIC_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    logger: logging.Logger | None = None,
) -> ChannelResult:
    """Discover the best team-selection video for a single channel."""

    logger = logger or logging.getLogger(__name__)
    collector = FPLVideoCollector(
        max_per_channel=max_per_channel, days_back=days_back, logger=logger
    )
    videos_by_channel = collector.collect_videos_by_channel([channel_url])
    videos = videos_by_channel.get(channel_url, [])

    if not videos:
        raise VideoPickerError(f"No recent videos found for {channel_name}")

    heuristic_filter = HeuristicFilter(gameweek=gameweek, logger=logger)
    candidates, _notes = heuristic_filter.filter_and_rank(
        videos, max_candidates=max_per_channel
    )
    if not candidates:
        raise VideoPickerError(
            f"No videos passed heuristic filtering for {channel_name}"
        )

    selected_video = candidates[0]
    confidence = 0.0
    reasoning = "Heuristic fallback - top scored candidate"
    matched_signals: list[str] = []

    try:
        ranker = AnthropicRanker(
            model=anthropic_model, temperature=temperature, logger=logger
        )
        ranked = ranker.rank_videos_by_channel({channel_url: candidates}, gameweek)
        if ranked and channel_url in ranked:
            selected_video, anthropic_response = ranked[channel_url]
            confidence = anthropic_response.confidence
            reasoning = anthropic_response.reasoning
            matched_signals = anthropic_response.matched_signals
    except Exception as exc:  # pragma: no cover - fallback safety
        logger.debug("Anthropic ranking unavailable, using heuristic result: %s", exc)

    heuristic_score = heuristic_filter.calculate_score(selected_video)
    alternatives = [video for video in candidates if video is not selected_video][:3]

    return ChannelResult(
        channel_name=channel_name,
        channel_url=channel_url,
        picked=selected_video,
        alternatives=alternatives,
        confidence=confidence,
        reasoning=reasoning,
        matched_signals=matched_signals,
        heuristic_score=heuristic_score,
    )


class FPLVideoCollector:  # pragma: no cover - hits live YouTube Data API
    """Handles YouTube video collection with resilient fetching."""

    DEFAULT_CHANNELS: ClassVar[list[str]] = [
        "https://www.youtube.com/@FPLRaptor",
        "https://www.youtube.com/channel/UCweDAlFm2LnVcOqaFU4_AGA",  # FPL Mate
        "https://www.youtube.com/channel/UCxeOc7eFxq37yW_Nc-69deA",  # FPL Andy
        "https://www.youtube.com/fplfocal",
        "https://www.youtube.com/channel/UCcPWnCj5AKC19HaySZjb25g",  # FP Harry
    ]

    def __init__(
        self,
        max_per_channel: int = 6,
        days_back: int = 14,
        logger: logging.Logger | None = None,
    ):
        self.max_per_channel = max_per_channel
        self.days_back = days_back
        self.logger = logger or logging.getLogger(__name__)
        self.cutoff_date = datetime.now(UTC) - timedelta(days=days_back)

    def _resolve_channel_id(self, yt_service, channel_url: str) -> str | None:
        """Resolve various YouTube channel URL formats to channel ID."""
        try:
            if "@" in channel_url:
                # Handle URL: https://www.youtube.com/@FPLRaptor
                handle = channel_url.split("@")[1]
                response = (
                    yt_service.channels()
                    .list(part="id", forHandle=f"@{handle}")
                    .execute()
                )

                if response.get("items"):
                    return response["items"][0]["id"]

            elif "channel/" in channel_url:
                # Channel ID URL: https://www.youtube.com/channel/UCweDAlFm2LnVcOqaFU4_AGA
                return channel_url.split("channel/")[1]

            else:
                # Custom URL: https://www.youtube.com/fplfocal
                # Extract the identifier after the last slash
                search_query = channel_url.split("/")[-1]

                # Try search API to find the channel
                search_response = (
                    yt_service.search()
                    .list(part="snippet", type="channel", q=search_query, maxResults=5)
                    .execute()
                )

                if search_response.get("items"):
                    # Return the first result's channel ID
                    return search_response["items"][0]["snippet"]["channelId"]

        except HttpError as e:
            self.logger.error(f"API error resolving channel ID for {channel_url}: {e}")
        except Exception as e:
            self.logger.error(f"Error resolving channel ID for {channel_url}: {e}")

        return None

    @retry(
        retry=retry_if_exception_type((ConnectionError, TimeoutError, HttpError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
    )
    def _fetch_channel_videos(self, channel_url: str) -> list[VideoItem]:
        """Fetch videos from a single channel using YouTube Data API v3."""
        videos = []
        api_key = os.environ.get("YOUTUBE_API_KEY")
        if not api_key:
            self.logger.error("YOUTUBE_API_KEY environment variable not found")
            return []

        try:
            yt = build("youtube", "v3", developerKey=api_key)

            # Resolve channel URL to channel ID
            channel_id = self._resolve_channel_id(yt, channel_url)
            if not channel_id:
                self.logger.error(f"Could not resolve channel ID for: {channel_url}")
                return []

            # Get channel info and uploads playlist ID
            channel_response = (
                yt.channels()
                .list(part="snippet,contentDetails", id=channel_id)
                .execute()
            )

            if not channel_response.get("items"):
                self.logger.error(f"Channel not found: {channel_id}")
                return []

            channel_info = channel_response["items"][0]
            channel_name = channel_info["snippet"]["title"]
            uploads_playlist_id = channel_info["contentDetails"]["relatedPlaylists"][
                "uploads"
            ]

            self.logger.debug(f"Fetching videos from {channel_name} (ID: {channel_id})")

            # Fetch videos from uploads playlist
            all_videos = []
            next_page_token = None

            while len(all_videos) < self.max_per_channel:
                videos_response = (
                    yt.playlistItems()
                    .list(
                        part="snippet,contentDetails",
                        playlistId=uploads_playlist_id,
                        maxResults=min(50, self.max_per_channel - len(all_videos)),
                        pageToken=next_page_token,
                    )
                    .execute()
                )

                items = videos_response.get("items", [])
                if not items:
                    break

                for item in items:
                    try:
                        # Parse published date
                        published_str = item["snippet"]["publishedAt"]
                        # YouTube API returns RFC 3339 format: 2024-08-23T15:30:00Z
                        published_at = datetime.fromisoformat(
                            published_str.replace("Z", "+00:00")
                        )

                        # Skip videos that are too old
                        if published_at < self.cutoff_date:
                            continue

                        video_id = item["contentDetails"]["videoId"]
                        video_url = f"https://www.youtube.com/watch?v={video_id}"

                        video_item = VideoItem(
                            title=item["snippet"]["title"],
                            url=video_url,
                            published_at=published_at,
                            channel_name=channel_name,
                            description=item["snippet"]["description"] or "",
                        )

                        all_videos.append(video_item)
                        self.logger.debug(f"  Added: {video_item.title[:60]}...")

                    except (ValueError, KeyError) as e:
                        self.logger.debug(
                            f"  Skipped video {item.get('snippet', {}).get('title', 'Unknown')}: {e}"
                        )
                        continue

                next_page_token = videos_response.get("nextPageToken")
                if not next_page_token:
                    break

            videos = all_videos[: self.max_per_channel]

        except HttpError as e:
            self.logger.error(f"YouTube API error for {channel_url}: {e}")
            return []
        except Exception as e:
            self.logger.error(f"Failed to fetch from {channel_url}: {e}")
            return []

        self.logger.info(f"Collected {len(videos)} videos from {channel_name}")
        return videos

    def collect_videos_by_channel(
        self, channel_urls: list[str]
    ) -> dict[str, list[VideoItem]]:
        """Collect videos from all provided channels, grouped by channel."""
        videos_by_channel = {}
        total_videos = 0

        for channel_url in channel_urls:
            videos = self._fetch_channel_videos(channel_url)
            if videos:
                # Sort each channel's videos by publication date (newest first)
                videos.sort(key=lambda v: v.published_at, reverse=True)
                videos_by_channel[channel_url] = videos
                total_videos += len(videos)
            else:
                videos_by_channel[channel_url] = []

        self.logger.info(
            f"Total videos collected: {total_videos} across {len(videos_by_channel)} channels"
        )
        return videos_by_channel


class HeuristicFilter:  # pragma: no cover - exercised indirectly via select_single_channel
    """Pre-filters videos using keyword-based heuristics."""

    POSITIVE_KEYWORDS: ClassVar[list[str]] = [
        "team selection",
        "my team",
        "team reveal",
        "final team",
        "gw",
        "gameweek",
        "wildcard",
        "free hit",
        "bench boost",
        "triple captain",
        "draft",
        "picks",
        "starting xi",
    ]

    NEGATIVE_KEYWORDS: ClassVar[list[str]] = [
        "deadline stream",
        "watchalong",
        "price change",
        "press conference",
        "reaction",
        "news",
        "match reaction",
        "live stream",
        "q&a",
        "chat",
    ]

    def __init__(
        self, gameweek: int | None = None, logger: logging.Logger | None = None
    ):
        self.gameweek = gameweek
        self.logger = logger or logging.getLogger(__name__)

    def extract_gameweek_number(self, text: str) -> int | None:
        """Extract gameweek number from text."""
        gw_patterns = [r"\bgw\s*(\d+)\b", r"\bgameweek\s*(\d+)\b", r"\bgw(\d+)\b"]

        for pattern in gw_patterns:
            match = re.search(pattern, text.lower())
            if match:
                return int(match.group(1))
        return None

    def calculate_score(self, video: VideoItem) -> float:
        """Calculate heuristic score for a video."""
        score = 0.0
        text = f"{video.normalized_title} {video.description.lower()}"

        # Positive keywords boost
        for keyword in self.POSITIVE_KEYWORDS:
            if keyword in text:
                if keyword in ["team selection", "team reveal", "final team"]:
                    score += 3.0  # Strong indicators
                elif keyword in ["gw", "gameweek"]:
                    score += 2.0
                else:
                    score += 1.0

        # Negative keywords reduce score
        for keyword in self.NEGATIVE_KEYWORDS:
            if keyword in text:
                score -= 2.0

        # Gameweek-specific bonus
        if self.gameweek:
            detected_gw = self.extract_gameweek_number(text)
            if detected_gw == self.gameweek:
                score += 5.0  # Strong bonus for matching gameweek
            elif detected_gw and abs(detected_gw - self.gameweek) <= 1:
                score += 1.0  # Small bonus for adjacent gameweeks

        # Recency bonus (newer videos get slight preference)
        days_old = (datetime.now(UTC) - video.published_at).days
        if days_old <= 1:
            score += 1.0
        elif days_old <= 3:
            score += 0.5

        return max(0.0, score)  # Don't allow negative scores

    def filter_and_rank(
        self, videos: list[VideoItem], max_candidates: int = 12
    ) -> tuple[list[VideoItem], dict[str, Any]]:
        """Filter videos and return top candidates with scoring details."""
        scored_videos = []

        for video in videos:
            score = self.calculate_score(video)
            if score > 0:  # Only keep videos with positive scores
                scored_videos.append((video, score))

        # Sort by score descending, then by recency
        scored_videos.sort(key=lambda x: (x[1], x[0].published_at), reverse=True)

        # Take top candidates
        top_candidates = [video for video, _ in scored_videos[:max_candidates]]

        heuristic_notes = {
            "rules": {
                "positive_keywords": self.POSITIVE_KEYWORDS,
                "negative_keywords": self.NEGATIVE_KEYWORDS,
                "gameweek_filter": self.gameweek,
            },
            "scores": {
                video.url: score for video, score in scored_videos[:max_candidates]
            },
        }

        self.logger.info(
            f"Heuristic filter: {len(top_candidates)} candidates from {len(videos)} videos"
        )
        return top_candidates, heuristic_notes


class AnthropicRanker:  # pragma: no cover - requires live Anthropic API interaction
    """Uses Anthropic Claude to intelligently rank video candidates."""

    def __init__(
        self,
        model: str = DEFAULT_ANTHROPIC_MODEL,
        temperature: float = DEFAULT_TEMPERATURE,
        logger: logging.Logger | None = None,
    ):
        self.model = model
        self.temperature = temperature
        self.logger = logger or logging.getLogger(__name__)

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable is required")

        self.client = anthropic.Anthropic(api_key=api_key)

    def _create_batch_prompt(
        self, channels_candidates: dict[str, list[VideoItem]], gameweek: int | None
    ) -> str:
        """Create the prompt for Claude to rank videos for all channels."""
        channels_json = {}

        for _channel_url, candidates in channels_candidates.items():
            if not candidates:
                continue

            channel_name = candidates[0].channel_name
            candidates_json = []

            for i, video in enumerate(candidates):
                candidates_json.append(
                    {
                        "index": i,
                        "title": video.title,
                        "url": video.url,
                        "published_at": video.published_at.isoformat(),
                        "desc": video.description[:200] + "..."
                        if len(video.description) > 200
                        else video.description,
                    }
                )

            channels_json[channel_name] = candidates_json

        gameweek_context = f" for gameweek {gameweek}" if gameweek else ""

        return f"""You are an FPL content classifier. Return ONLY valid JSON with no extra commentary.

For each YouTube channel, analyze their videos to find the most likely FPL "team selection" video{gameweek_context}:

{json.dumps(channels_json, indent=2)}

FPL "team selection" videos typically have titles like:
- "My GW5 Team Selection"
- "Gameweek 5 Team Reveal"
- "Final Team GW5"
- "Wildcard Draft GW5"
- "Free Hit Team"

Return this exact JSON schema with one result per channel:
```json
{{
  "channels": [
    {{
      "channel_name": "FPL Mate",
      "chosen_index": 0,
      "chosen_url": "https://youtube.com/watch?v=...",
      "confidence": 0.85,
      "matched_signals": ["team selection", "gw5"],
      "reasoning": "Clear team selection video with gameweek number"
    }}
  ]
}}
```

Analyze each channel separately and choose the best team selection video for each."""

    def rank_videos_by_channel(
        self,
        channels_candidates: dict[str, list[VideoItem]],
        gameweek: int | None = None,
    ) -> dict[str, tuple[VideoItem, AnthropicChannelResponse] | None]:
        """Use Claude to rank videos for each channel and return the best choices."""
        results = {}

        # Filter out channels with no candidates
        valid_channels = {
            url: candidates
            for url, candidates in channels_candidates.items()
            if candidates
        }

        if not valid_channels:
            return {}

        try:
            prompt = self._create_batch_prompt(valid_channels, gameweek)

            total_candidates = sum(
                len(candidates) for candidates in valid_channels.values()
            )
            self.logger.debug(
                f"Calling Anthropic API with {total_candidates} candidates across {len(valid_channels)} channels"
            )

            message = self.client.messages.create(
                model=self.model,
                max_tokens=2000,  # Increased for multiple channels
                temperature=self.temperature,
                system="You are an FPL content classifier. You strictly return only valid JSON and no extra commentary.",
                messages=[{"role": "user", "content": prompt}],
            )

            response_text = message.content[0].text.strip()
            self.logger.debug(f"Raw API response: {response_text}")

            # Parse JSON response
            parsed_response = self._parse_batch_response(response_text, valid_channels)
            if not parsed_response:
                return {}

            # Map responses back to channels
            for channel_response in parsed_response.channels:
                # Find the channel URL by matching channel name
                matching_channel_url = None
                for url, candidates in valid_channels.items():
                    if (
                        candidates
                        and candidates[0].channel_name == channel_response.channel_name
                    ):
                        matching_channel_url = url
                        break

                if matching_channel_url and 0 <= channel_response.chosen_index < len(
                    valid_channels[matching_channel_url]
                ):
                    chosen_video = valid_channels[matching_channel_url][
                        channel_response.chosen_index
                    ]
                    results[matching_channel_url] = (chosen_video, channel_response)
                    self.logger.info(
                        f"Claude selected for {channel_response.channel_name}: {chosen_video.title[:60]}... (confidence: {channel_response.confidence})"
                    )
                else:
                    self.logger.warning(
                        f"Could not map response for channel: {channel_response.channel_name}"
                    )

            return results

        except Exception as e:
            self.logger.error(f"Anthropic API error: {e}")
            # Fallback to heuristic selections
            return {}

    def _parse_batch_response(
        self, response_text: str, channels_candidates: dict[str, list[VideoItem]]
    ) -> AnthropicResponse | None:
        """Parse and validate Claude's JSON response for batch processing."""
        try:
            # Remove code fences if present
            if "```" in response_text:
                json_match = re.search(
                    r"```(?:json)?\s*(\{.*?\})\s*```", response_text, re.DOTALL
                )
                if json_match:
                    response_text = json_match.group(1)

            data = json.loads(response_text)
            response = AnthropicResponse(**data)

            # Validate each channel response
            for channel_response in response.channels:
                # Find matching channel
                matching_candidates = None
                for candidates in channels_candidates.values():
                    if (
                        candidates
                        and candidates[0].channel_name == channel_response.channel_name
                    ):
                        matching_candidates = candidates
                        break

                if not matching_candidates:
                    self.logger.error(
                        f"No candidates found for channel: {channel_response.channel_name}"
                    )
                    continue

                # Validate chosen_index and chosen_url
                if not (0 <= channel_response.chosen_index < len(matching_candidates)):
                    self.logger.error(
                        f"Invalid chosen_index {channel_response.chosen_index} for {channel_response.channel_name}"
                    )
                    continue

                expected_url = matching_candidates[channel_response.chosen_index].url
                if channel_response.chosen_url != expected_url:
                    self.logger.error(
                        f"URL mismatch for {channel_response.channel_name}: got {channel_response.chosen_url}, expected {expected_url}"
                    )
                    continue

            return response

        except (json.JSONDecodeError, ValidationError, KeyError) as e:
            self.logger.error(f"Failed to parse Anthropic response: {e}")
            return None


def load_channels_from_file(
    file_path: str, logger: logging.Logger
) -> list[str]:  # pragma: no cover - CLI helper
    """Load channel URLs from a text file."""
    try:
        with Path(file_path).open() as f:
            channels = []
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    channels.append(line)
        logger.info(f"Loaded {len(channels)} channels from {file_path}")
        return channels
    except FileNotFoundError:
        logger.error(f"Channels file not found: {file_path}")
        return []


def load_channels_config(
    config_path: str, logger: logging.Logger
) -> dict[str, dict[str, str]]:  # pragma: no cover - CLI helper
    """Load channels from JSON config file."""
    try:
        with Path(config_path).open() as f:
            config = json.load(f)
        channels = {}
        for channel in config["channels"]:
            channels[channel["name"]] = {
                "url": channel["url"],
                "description": channel.get("description", ""),
            }
        logger.info(
            f"Loaded {len(channels)} channels from config: {', '.join(channels.keys())}"
        )
        return channels
    except FileNotFoundError:
        logger.error(f"Channels config file not found: {config_path}")
        return {}
    except (json.JSONDecodeError, KeyError) as e:
        logger.error(f"Invalid channels config format: {e}")
        return {}


def setup_logging(verbose: bool = False) -> logging.Logger:  # pragma: no cover - CLI helper
    """Setup logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    return logging.getLogger(__name__)


def cli_main(argv: Sequence[str] | None = None) -> int:  # pragma: no cover - CLI wrapper
    """Main entry point for the FPL video picker script."""
    parser = argparse.ArgumentParser(
        description="Find the most likely FPL team selection video from YouTube channels",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --gameweek 5
  %(prog)s --max-per-channel 10 --out results.json
  %(prog)s --channels-file my_channels.txt --verbose
  %(prog)s --single-channel "FPL Raptor" --gameweek 2 --out raptor.json

Returns one selected video per channel, analyzing each channel's videos individually.
Use --single-channel for parallel processing in shell scripts.
        """,
    )

    parser.add_argument(
        "--channels-file", help="Path to file containing channel URLs (one per line)"
    )
    parser.add_argument(
        "--single-channel",
        help="Process only a single channel by name (requires --channels-config)",
    )
    parser.add_argument(
        "--channels-config",
        default=str(DEFAULT_CHANNELS_CONFIG),
        help="Path to channels.json config file (default: project youtube-titles/channels.json)",
    )
    parser.add_argument(
        "--max-per-channel",
        type=int,
        default=6,
        help="Maximum videos to fetch per channel (default: 6)",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=14,
        help="Only consider videos from the last N days (default: 14)",
    )
    parser.add_argument(
        "--gameweek", "-gw", type=int, help="Target gameweek number for selection bias"
    )
    parser.add_argument(
        "--anthropic-model",
        default=DEFAULT_ANTHROPIC_MODEL,
        help=f"Anthropic model to use (default: {DEFAULT_ANTHROPIC_MODEL})",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=DEFAULT_TEMPERATURE,
        help="Temperature for Anthropic API (default: 0.0)",
    )
    parser.add_argument("--out", help="Path to write JSON results file")
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable debug logging"
    )

    args = parser.parse_args(argv)

    # Setup logging
    logger = setup_logging(args.verbose)
    logger.info("Starting FPL Video Picker")

    try:
        # Load channels
        if args.single_channel:
            # Single channel mode
            channels_config = load_channels_config(args.channels_config, logger)
            if not channels_config:
                return 1

            if args.single_channel not in channels_config:
                logger.error(
                    f"Channel '{args.single_channel}' not found in config. Available: {', '.join(channels_config.keys())}"
                )
                return 1

            channels = [channels_config[args.single_channel]["url"]]
            logger.info(f"Processing single channel: {args.single_channel}")

        elif args.channels_file:
            channels = load_channels_from_file(args.channels_file, logger)
            if not channels:
                return 1
        else:
            channels = FPLVideoCollector.DEFAULT_CHANNELS
            logger.info(f"Using {len(channels)} default channels")

        # Collect videos by channel
        collector = FPLVideoCollector(
            max_per_channel=args.max_per_channel, days_back=args.days, logger=logger
        )
        videos_by_channel = collector.collect_videos_by_channel(channels)

        if not any(videos_by_channel.values()):
            logger.error("No videos found matching criteria")
            return 2

        # Process each channel individually
        heuristic_filter = HeuristicFilter(gameweek=args.gameweek, logger=logger)
        channel_results = []
        channels_candidates = {}
        heuristic_notes = {}

        # Apply heuristic filtering per channel
        for channel_url, videos in videos_by_channel.items():
            if not videos:
                channel_name = channel_url.split("/")[-1]
                channel_results.append(
                    ChannelResult(
                        channel_name=channel_name,
                        channel_url=channel_url,
                        picked=None,
                        alternatives=[],
                    )
                )
                continue

            candidates, channel_heuristic_notes = heuristic_filter.filter_and_rank(
                videos, max_candidates=6
            )
            channels_candidates[channel_url] = candidates
            heuristic_notes[videos[0].channel_name] = channel_heuristic_notes

            logger.info(
                f"{videos[0].channel_name}: {len(candidates)} candidates after filtering"
            )

        # Rank with Anthropic (batch processing)
        anthropic_results = {}
        try:
            ranker = AnthropicRanker(
                model=args.anthropic_model, temperature=args.temperature, logger=logger
            )

            anthropic_results = ranker.rank_videos_by_channel(
                channels_candidates, args.gameweek
            )

        except Exception as e:
            logger.warning(f"Anthropic ranking failed: {e}, using heuristic fallback")

        # Create channel results
        for _channel_url, candidates in channels_candidates.items():
            if not candidates:
                continue

            channel_name = candidates[0].channel_name

            # Check if we have Anthropic results for this channel
            if channel_url in anthropic_results:
                selected_video, anthropic_response = anthropic_results[channel_url]
                confidence = anthropic_response.confidence
                reasoning = anthropic_response.reasoning
                matched_signals = anthropic_response.matched_signals
            else:
                # Fallback to heuristic choice (top candidate)
                selected_video = candidates[0]
                confidence = 0.0
                reasoning = "Heuristic fallback - top scored candidate"
                matched_signals = []

            # Calculate heuristic score for the selected video
            heuristic_score = heuristic_filter.calculate_score(selected_video)

            channel_results.append(
                ChannelResult(
                    channel_name=channel_name,
                    channel_url=channel_url,
                    picked=selected_video,
                    alternatives=candidates[1:4]
                    if selected_video == candidates[0]
                    else candidates[:3],  # Top 3 alternatives
                    confidence=confidence,
                    reasoning=reasoning,
                    matched_signals=matched_signals,
                    heuristic_score=heuristic_score,
                )
            )

        if not channel_results:
            logger.error("No results generated for any channel")
            return 2

        # Create result
        result = SelectionResult(
            channel_results=channel_results,
            gameweek=args.gameweek,
            generated_at=datetime.now(UTC),
            model=args.anthropic_model,
            heuristic_notes=heuristic_notes,
        )

        # Print human-readable summary
        if args.single_channel:
            # Single channel mode - simplified output
            channel_result = channel_results[0]
            if channel_result.picked:
                print("\n🎯 SELECTED VIDEO:")
                print(f"Channel: {channel_result.channel_name}")
                print(f"Title: {channel_result.picked.title}")
                print(f"URL: {channel_result.picked.url}")
                print(
                    f"Video ID: {channel_result.picked.url.split('v=')[1] if 'v=' in channel_result.picked.url else 'N/A'}"
                )
                print(f"Confidence: {channel_result.confidence:.2f}")
                print(
                    f"Published: {channel_result.picked.published_at.strftime('%Y-%m-%d %H:%M UTC')}"
                )
            else:
                print(f"\n❌ NO VIDEO FOUND for {channel_result.channel_name}")
        else:
            # Multi-channel mode - existing output format
            print("\n🎯 SELECTED VIDEOS BY CHANNEL:")
            print(f"{'=' * 80}")

            for i, channel_result in enumerate(channel_results, 1):
                if channel_result.picked:
                    print(f"\n{i}. {channel_result.channel_name}:")
                    print(f"   Title: {channel_result.picked.title}")
                    print(
                        f"   Published: {channel_result.picked.published_at.strftime('%Y-%m-%d %H:%M UTC')}"
                    )
                    print(f"   URL: {channel_result.picked.url}")
                    print(
                        f"   Confidence: {channel_result.confidence:.2f} | Heuristic Score: {channel_result.heuristic_score:.1f}"
                    )
                    print(f"   Reasoning: {channel_result.reasoning}")
                    if channel_result.matched_signals:
                        print(
                            f"   Signals: {', '.join(channel_result.matched_signals)}"
                        )
                else:
                    print(
                        f"\n{i}. {channel_result.channel_name}: No suitable videos found"
                    )

            print(f"\n{'=' * 80}")
            successful_selections = len([r for r in channel_results if r.picked])
            print(
                f"Successfully selected videos from {successful_selections}/{len(channel_results)} channels"
            )

        # Write JSON output if requested
        if args.out:
            if args.single_channel:
                # Single channel mode - simplified JSON
                channel_result = channel_results[0]
                if channel_result.picked:
                    # Extract video ID from URL
                    video_id = "N/A"
                    if "v=" in channel_result.picked.url:
                        video_id = channel_result.picked.url.split("v=")[1].split("&")[
                            0
                        ]

                    output_data = {
                        "channel_name": channel_result.channel_name,
                        "video_id": video_id,
                        "title": channel_result.picked.title,
                        "url": channel_result.picked.url,
                        "confidence": channel_result.confidence,
                        "published_at": channel_result.picked.published_at.isoformat(),
                        "published_at_formatted": channel_result.picked.published_at.strftime(
                            "%Y-%m-%d %H:%M UTC"
                        ),
                        "reasoning": channel_result.reasoning,
                        "matched_signals": channel_result.matched_signals,
                        "gameweek": result.gameweek,
                        "generated_at": result.generated_at.isoformat(),
                    }
                else:
                    output_data = {
                        "channel_name": channel_result.channel_name,
                        "video_id": None,
                        "title": None,
                        "url": None,
                        "confidence": 0.0,
                        "error": "No suitable video found",
                        "gameweek": result.gameweek,
                        "generated_at": result.generated_at.isoformat(),
                    }
            else:
                # Multi-channel mode - existing JSON format
                output_data = {
                    "channels": [
                        {
                            "channel_name": cr.channel_name,
                            "channel_url": cr.channel_url,
                            "picked": {
                                "title": cr.picked.title,
                                "url": cr.picked.url,
                                "published_at": cr.picked.published_at.isoformat(),
                                "description": cr.picked.description,
                            }
                            if cr.picked
                            else None,
                            "alternatives": [
                                {
                                    "title": video.title,
                                    "url": video.url,
                                    "published_at": video.published_at.isoformat(),
                                    "description": video.description,
                                }
                                for video in cr.alternatives
                            ],
                            "confidence": cr.confidence,
                            "reasoning": cr.reasoning,
                            "matched_signals": cr.matched_signals,
                            "heuristic_score": cr.heuristic_score,
                        }
                        for cr in channel_results
                    ],
                    "gameweek": result.gameweek,
                    "generated_at": result.generated_at.isoformat(),
                    "model": result.model,
                    "heuristic_notes": result.heuristic_notes,
                }

            with Path(args.out).open("w") as f:
                json.dump(output_data, f, indent=2)
            logger.info(f"Results written to {args.out}")

        return 0

    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1


__all__ = [
    "ChannelResult",
    "SelectionResult",
    "VideoItem",
    "VideoPickerError",
    "cli_main",
    "select_single_channel",
    "setup_logging",
]
